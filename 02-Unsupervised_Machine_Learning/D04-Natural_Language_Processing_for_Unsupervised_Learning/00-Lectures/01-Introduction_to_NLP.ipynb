{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "\n",
    "## What you'll learn in this course üßêüßê\n",
    "\n",
    "This course is an introduction to statistical analysis of text and language. It is a discipline that has recently undergone enormous scientific progress thanks to developments in deep learning and the exponential growth of computing power. In this course, We will start  with some basic NLP principles. We will learn: \n",
    "\n",
    "* Types of language encoding \n",
    "* Basic text preprocessing with N-Grams\n",
    "* Basic analysis of frequency with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What is NLP? ü¶úü¶ú\n",
    "\n",
    "Natural Language Processing, often abbreviated as *NLP*  consists in studying how computer languages and human languages (natural language) interact.\n",
    "\n",
    "### Examples of applications ‚úÖ\n",
    "\n",
    "NLP has grown enormously since its beginnings in the 1950s thanks to the rapid growth of computing power and progress in deep learning. Here are a few of popular NLP applications:\n",
    "\n",
    "\n",
    "* Spell checking, keyword searching, synonym searching üéØ\n",
    "\n",
    "* Extraction of information from websites (e.g. product prices, dates, names of people, companies, etc...) ‚ÑπÔ∏è\n",
    "\n",
    "* Classification:\n",
    "    * Sentiment Analysis üíñ\n",
    "    * Topic Modeling üóº\n",
    "\n",
    "* Translation üåç\n",
    "\n",
    "* Speech-controlled system üí¨\n",
    "\n",
    "\n",
    "### Why language a very special type of data üóº\n",
    "\n",
    "Most of the data that Data Scientists is usually quantitative (stock price, revenue etc.) or qualitative (Color, gender etc.). With NLP, we deal with whole corpus of text and we need to somehow translate that into numbers so that computers can understand it. This comes with a set challenges to keep in mind: \n",
    "\n",
    "\n",
    "  * Language is an ambiguous mode of expression. The same word, sentence or text can have completely different interpretations. Generations of researchers continue to be interested in works, quotations and even simple words, so rich is the richness of language interpretation.\n",
    "\n",
    "  * Interpretation of language depends on situational context, the surrounding real world, common sense, and cultural and social norms.\n",
    "\n",
    "All this makes NLP an infinitely interesting subject where data-scientists have made tremendous progress. üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing in NLP üößüöß\n",
    "\n",
    "\n",
    "In order to analyze text with Python, it is necessary to work on preprocessing data so that it can be easily understood by a computer. Let's quickly show different word processing technics that can be used for Machine Learning:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>\n",
    "    Preprocessing type\n",
    "</th>\n",
    "<th>\n",
    "    Description\n",
    "</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Lower case\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "Python, like many other computer languages, distinguishes between upper and lower case letters. Thus, *A* is not the same character as *a*. Similarly, the words *Learn* and *learn*, although understood the same way by our human brains, do not represent the same strings of characters in a language like Python. We will therefore generally replace all characters in a text with their lowercase equivalent before starting more advanced processing.\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Punctuation\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "It's rare to analyze punctuation when it comes to text mining. We will usually remove all punctuation so that we only have words to analyze.\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Stop words\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Stop_word\" target=\"_blank\">Stop words</a> corresponds to all the linking words, articles and quantifiers that are used a lot in a language but are not in themselves meaningful. For example, in English these words are: *a*, *the*, *and*, *any* etc.\n",
    "\n",
    "<br/>\n",
    "\n",
    "They are generally removed because they are so frequent in texts that they generally prevent the different patterns that can be used to detect the words that really characterize a text.\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Common words\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "For some analyses, we will remove common words. For example, if you want to perform topic modeling related to traveling in Italy, you will probably remove the vocabulary that commonly describes Italy so that your algorithm will focus on the real differences between topics.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Rare words\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "Conversely, words that are too infrequent in texts may be useless because their connection with other words in the text could create noise.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Typos\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "In order to unify spelling, Python packages are often used to correct typos.\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Stemming (Rooting)\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "Many words in different languages are simple a variation of a common root. This is  called an *inflection*. <a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\">Stemming</a> chops off inflections in order to keep only the common root of the words and thus be able to make analyses that were previously impossible. For example: \n",
    "\n",
    "* Cats ‚Üí Cat\n",
    "* Caresses ‚Üí Caress \n",
    "* Ponies ‚Üí Poni\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Lemmatization\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "<a href=\"https://queryunderstanding.com/stemming-and-lemmatization-6c086742fe45\" target=\"_blank\"> Lemmatization</a> is a \"clever stemming\". Instead of simply choping off inflections, it will transform words based on lexical knowledge. For example, stemming would perform these transformations: \n",
    "\n",
    "*   Car ‚Üí Car\n",
    "*   Cars ‚Üí Car \n",
    "*   Caring ‚Üí Car\n",
    "*   Care ‚Üí Car\n",
    "\n",
    "Whereas Lemmatization would do something smarter: \n",
    "\n",
    "*   Car ‚Üí Car \n",
    "*   Cars ‚Üí Car \n",
    "*   Caring ‚Üí Care \n",
    "*   Care ‚Üí Care \n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams üèòÔ∏è\n",
    "\n",
    "When you will be doing text mining, you will need to seperate a document into groups of words. Thus, we won't let the computer deal with a string of characters of the form:\n",
    "\n",
    "\"The cat eats fish.\"\n",
    "\n",
    "Instead, a document is usually represented as *N-grams*. N-grams are a way of breaking down the text into groups of \"N\" words. For example, a 1-grams or unigrams of the previous text looks like this:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td>‚Äúthe‚Äù\n",
    "   </td>\n",
    "   <td>‚Äúcat‚Äù\n",
    "   </td>\n",
    "   <td>‚Äúeats‚Äù\n",
    "   </td>\n",
    "   <td>‚Äúthe‚Äù\n",
    "   </td>\n",
    "   <td>‚Äúfish‚Äù\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "2-grams would be:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <td>‚Äúthe cat‚Äù\n",
    "   </td>\n",
    "   <td>‚Äúcat eats‚Äù\n",
    "   </td>\n",
    "   <td>‚Äúeats the‚Äù\n",
    "   </td>\n",
    "   <td>‚Äúthe fish‚Äù\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "N-grams is a very useful tool for analysing the general idea of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parse trees üå≥\n",
    "\n",
    "Parse trees are tools that allow you to break down simple sentences automatically.\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=15yqMknQsiaqnubxCQjN4AwnlmcDWRQh1)\n",
    "\n",
    "\n",
    "To build sentences, we use grammatical rules. \n",
    "\n",
    "For example, a sentence can be composed of a nominal group (Noun phrase *NP*) followed by a verbal group (*Verb phrase*). These rules can be listed, and we can use Python to analyze simple sentences. \n",
    "\n",
    "It is thanks to this technology that Personal Digital Assistants such as Siri (Apple), Alexa (Amazon) or Cortana (Microsoft) are able to quickly understand vocal instructions after converting them into text. ü§ñ\n",
    "\n",
    "However, when sentences become too complex, this type of analysis does not allow the meaning to be extracted in a way that is intelligible to the computer. **It is better suited to understanding the meaning of short, precise and factual instructions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining ‚õèÔ∏è‚õèÔ∏è\n",
    "\n",
    "Before moving on to advanced Machine Learning methods for language comprehension, let's start by introducing techniques that are very frequently used in linguistics and quantitative marketing to extract information from textual data.\n",
    "\n",
    "\n",
    "### Term frequency üë®‚Äçüë©‚Äçüëß\n",
    "\n",
    "*Word frequency* is simply how many times a word occurs in a document: \n",
    "\n",
    "$$\n",
    "Term\\hspace{0.2cm}frequency = \\frac{word\\hspace{0.2cm} occurences}{total\\hspace{0.2cm}word\\hspace{0.2cm}occurences}\n",
    "$$\n",
    " \n",
    "For example in the following sentence: *The black cat eats fish but does not eat other black cats*. The frequency table can be written as follows after lemmization treatment and deletion of stop words:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "   <th>Term\n",
    "   </th>\n",
    "   <th>Occurrences\n",
    "   </th>\n",
    "   <th>Term Frequency\n",
    "   </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>‚Äúcat‚Äù\n",
    "   </td>\n",
    "   <td>2\n",
    "   </td>\n",
    "   <td>0.25\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>‚Äúblack‚Äù\n",
    "   </td>\n",
    "   <td>2\n",
    "   </td>\n",
    "   <td>0.25\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>‚Äúeat‚Äù\n",
    "   </td>\n",
    "   <td>2\n",
    "   </td>\n",
    "   <td>0.25\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>‚Äúfish‚Äù\n",
    "   </td>\n",
    "   <td>1\n",
    "   </td>\n",
    "   <td>0.125\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td>‚Äúother‚Äù\n",
    "   </td>\n",
    "   <td>1\n",
    "   </td>\n",
    "   <td>0.125\n",
    "   </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "   <td><strong>Total</strong>\n",
    "   </td>\n",
    "   <td><strong>8</strong>\n",
    "   </td>\n",
    "   <td><strong>1</strong>\n",
    "   </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inverse Document Frequency üôÉ\n",
    "\n",
    "Term frequency is usually too *simple* when you need to perform classification. Instead, we use Inverse Document Frequency (IDF) where a high IDF mean a representative unique word. Here is the formula:\n",
    "\n",
    "\n",
    "$$\n",
    "IDF = log(\\frac{N}{n})\n",
    "$$\n",
    "\n",
    "\n",
    "Where \n",
    "\n",
    "* $N$ is total number of documents in a corpus.\n",
    "* $n$ is total number of documents where a term $t$ appears in. \n",
    "\n",
    "For example, if we consider the following two documents: \n",
    "\n",
    "* *the black cat eats fish but does not eat other black cats.* \n",
    "* *the giraffe eats leaves thanks to its long neck but does not eat fish.* \n",
    "\n",
    "\n",
    "If we calculate the IDF on these examples, we get the following table:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Term\n",
    "    </th>\n",
    "    <th>$N$\n",
    "    </th>\n",
    "    <th>$n$\n",
    "    </th>\n",
    "    <th>IDF\n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"cat\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>0.30\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"black\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>0.30\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"eat\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"fish\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>‚Äúother‚Äù\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>0.30\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"giraffe\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>0.30\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"leaf\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>0.30\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"long\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>0.30\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\"neck\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>0.30\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency - Inverse document frequency üë®‚Äçüë©‚Äçüëß üôÉ\n",
    "\n",
    "*TF-IDF* is simply $TF \\times IDF$. This metric is a mix between $TF$ and $IDF$ and is widely used because it describes the importance of a word in a document as well as its frequency, which is extremely useful. \n",
    "\n",
    "For example, imagine a word that has a high frequency but that is contained in every document, its importance therefore becomes relative low. \n",
    "\n",
    "Let's take our documents and apply TF-IDF:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Document\n",
    "    </th>\n",
    "    <th>Term\n",
    "    </th>\n",
    "    <th>TF\n",
    "    </th>\n",
    "    <th>IDF\n",
    "    </th>\n",
    "    <th>TF-IDF\n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>\"cat\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.25</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.30</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.075</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>‚Äúblack‚Äù\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.25</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.30</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.075</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>‚Äúeats‚Äù\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.25</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>‚Äúfish‚Äù\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.125</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        1</p>\n",
    "    </td>\n",
    "    <td>‚Äúother‚Äù\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.125</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.30</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.037</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\"giraffe\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.30</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.06</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\"eats\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.30</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.30</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\"leaf\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.30</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.06</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\"long\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.30</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.06</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>\n",
    "        2</p>\n",
    "    </td>\n",
    "    <td>\"neck\"\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.2</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.30</p>\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>\n",
    "        0.06</p>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "Given this table, we understand that the most important words in document 1 that distinguish it from the other documents of the corpus are:\n",
    "\n",
    "* \"cat\" \n",
    "* \"black\"\n",
    "\n",
    "In document 2 the most important terms are:\n",
    "\n",
    "* \"giraffe\"\n",
    "* \"leaf\" \n",
    "* \"long\"\n",
    "* \"neck\" \n",
    "\n",
    "Conversely, the following words have a low tf-idf in both documents:\n",
    "\n",
    "* \"eat\"\n",
    "* \"fish\"\n",
    "\n",
    "We can therefore conclude that both documents talk about eating fish, but that document 1 focuses on cats, while document 2 focuses on giraffes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* <a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\">Stemming</a>\n",
    "* <a href=\"https://queryunderstanding.com/stemming-and-lemmatization-6c086742fe45\" target=\"_blank\"> Lemmatization</a> \n",
    "* <a href=\"https://en.wikipedia.org/wiki/Stop_word\" target=\"_blank\"> Stop Words</a>\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\">TF-IDF</a>\n",
    "* <a href=\"https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\" target=\"_blank\">TF-IDF from scratch in python on real world dataset</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
