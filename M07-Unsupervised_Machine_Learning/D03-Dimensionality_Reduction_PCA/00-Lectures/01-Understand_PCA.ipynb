{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction - PCA\n",
    "\n",
    "## What you'll learn in this course üßêüßê\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform\" target=\"_blank\">PCA</a> stands for *Principal Component Analysis*. The goal of this unsupervised algorithm is to create a linear combination of features that will transform your initial dataset into a smaller dataset. \n",
    "\n",
    "In this course, we will learn: \n",
    "\n",
    "* What is Dimensionality Reduction and why do we use it\n",
    "* What is PCA\n",
    "* What is a covariance matrix\n",
    "* What are eigen-values and eigen-vectors\n",
    "* What is SVD algorithm\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction ü¶äü¶ä\n",
    "\n",
    "There is this misconception in data science that the more explanatory variables we have at our disposal, the better the model we can derive from them. üôÖ‚Äç‚ôÄÔ∏è\n",
    "\n",
    "This is only **partially true**.\n",
    "\n",
    "The goal of a data scientist is not always to create a model that is an excellent predictor of reality. Regardless of their complexity, not all problems are similar to image classification. \n",
    "\n",
    "It is often a matter of **extracting relevant information** that can be understood by non-specialists, such as in sociology, economics and many other fields. üëå\n",
    "\n",
    "As always in data science, the important thing is to produce results that are useful in a given context. üöÄ Other times, we come up against technical constraints, because the equipment at our disposal is not very efficient, or because we want to better understand data with which we are not very familiar.\n",
    "\n",
    "In fact, it is essential to use techniques that allow you to summarize information in a relevant and simple way. These different techniques can be categorized as size reduction, starting with a large number of variables that form a large space and then moving to a smaller space.\n",
    "\n",
    "### Visualisation \n",
    "\n",
    "One use case of dimensionality reduction is to easily visualize data. If you take a 4-D dataset, you won't be able to draw a graph representing that data. \n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/4-D_viz.png\" alt=\"4D_viz\" width=\"50%\" />\n",
    "\n",
    "Instead, what you want is to have something like this that you can interpret üëçüëç\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/2-D_viz.png\" alt=\"2D_viz\" width=\"50%\" />\n",
    "\n",
    "### Noise Reduction \n",
    "\n",
    "Another reason why you could use Dimensionality Reduction algorithms is to reduce noise. ü§´ It is really common in Data Science to have redundant data, i.e data that describes a phenomenon the same way. \n",
    "\n",
    "For example, imagine that you have two variables like *Time of sleep* and *Tiredness score* (these are two imaginary variables). Well they obviously describe the same thing and therefore are extremely correlated. That is why you can remove one of them. \n",
    "\n",
    "The above example is rather obvious but you might have variables that are not that easy to spot in a dataset. \n",
    "\n",
    "Let's take another example, image that you have a dataset of blurry images, i.e images with a lot of noise, you can reduce this noise and make the images better simply by applying PCA for example. \n",
    "\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/noise_to_no_noise_digits.png\" alt=\"noise_to_no_noise\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!(https://vimeo.com/485918508)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA) üê£üê£\n",
    "\n",
    "\n",
    "### Goal of PCA\n",
    "\n",
    "Now that we have a better understand of why doing dimensionaly reduction, let's talk about the most famous algorithm for this kind of task: **PCA**. \n",
    "\n",
    "The general idea of PCA is to use the variables at our disposal and combine them linearly with each other to create a smaller dimensional space that best represents all the information.\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/why_do_PCA.png\" alt=\"why_do_PCA\"/>\n",
    "\n",
    "\n",
    "To do that, we are going to multiply our initial dataset with a matrix of vectors that are called **Eigen Vectors** that will give us our **principal components**.\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/PCA_logic.png\" alt=\"PCA_logic\"/>\n",
    "\n",
    "\n",
    "**Eigen Vectors** have special properties that we will explain later in the course. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!(https://vimeo.com/485919093)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process \n",
    "\n",
    "Now how do we find these **Eigen vectors**? Well, we need to do it in 4 steps that are the following: \n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/PCA_process.png\" alt=\"PCA_process\"/>\n",
    "\n",
    "\n",
    "Let's go through them step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization üìèüìè\n",
    "\n",
    "This is a classic step in Machine Learning, we won't go into too much details here. The whole idea is simply to apply this formula to all data points in your dataset: \n",
    "\n",
    "$$ \\frac{(x_i - \\mu)}{\\sigma} $$\n",
    "\n",
    "Where \n",
    "\n",
    "*   $\\mu$ is the mean \n",
    "*   $\\sigma$ is the standard deviation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!(https://vimeo.com/485919459)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Matrix üòçüòç\n",
    "\n",
    "### Reminders üå∫\n",
    "\n",
    "Let's talk about covariance matrix. PCA's assumption is that the information in a dataset is contained inside variables with high variance, that is why covariance is important to calculate. Here are a few reminders before going forward. \n",
    "\n",
    "* **Variance:** represents how observations are spread out within a variable. To give you a better idea. Here are two variables with high and low variance:\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/High_variance_VS_low_variance.png\" alt=\"High_vs_low_variance\"/>\n",
    "\n",
    "* **Covariance:** represents how two variables are related to each other. Here is an example of a covariance matrix\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/Covariance_matrix.png\" alt=\"Covariance Matrix\"/>\n",
    "\n",
    "On the **diagonal** are the variance of each variables whereas on the **off-diagonal** you have the covariance of each variable with each other. \n",
    "\n",
    "\n",
    "**How to interpret covariance?**ü§î \n",
    "\n",
    "* High covariance --> Variables are **statiscally dependent** (meaning there are redundancy between observations of variable $X$ and $Y$). It can be positive or negative, it doesn't matter.\n",
    "\n",
    "* Low covariance --> Variables are **statiscally independent** (meaning there are no redundancy between $X$ and $Y$). \n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/covariance_comparisons.png\" alt=\"covariance_comparison\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!(https://vimeo.com/485919778)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the whole goal of PCA is to summarize information. Therefore, in an ideal world, we would like to have a covariance matrix that only **has non-zero values on the diagonals**\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/ideal_covariance_matrix.png\" alt=\"ideal_covariance_matrix\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!(https://vimeo.com/485920648)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD üèóÔ∏èüèóÔ∏è\n",
    "\n",
    "### What is that?\n",
    "\n",
    "SVD stands for *Singular Value Decomposition*. It is the method that PCA is all based upon. It simply stands that: \n",
    "\n",
    "$$A = U \\Sigma V^\\intercal$$\n",
    "\n",
    "Where \n",
    "\n",
    "*   $A$ is a random matrix \n",
    "*   $U$ is another matrix composed of $Eigen-vectors$ of $AA^\\intercal$\n",
    "*   $\\Sigma$ is a diagonal matrix composed of $Eigen-values$ \n",
    "*   $V\\intercal$ is another matrix composed of $Eigen-vectors$ of $A^\\intercal A$ \n",
    "\n",
    "\n",
    "**NB: $\\intercal$ means transpose. Check this link if you need to learn more üëâüëâ <a href=\"https://www.mathsisfun.com/definitions/transpose-matrix-.html#:~:text=%22Flipping%22%20a%20matrix%20over%20its,rows%20and%20columns%20get%20swapped.&text=Example%3A%20the%20value%20in%20the,3rd%20row%20and%201st%20column.\" target=\"_blank\">Math is fun</a> **\n",
    "\n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/SVD.png\" alt=\"SVD\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen Vectors & Eigen Values \n",
    "\n",
    "Let's now tackle the concept of $Eigen-vectors$ and $Eigen-values$. Up until this point, we didn't need to know what it was, but now we do! ü§ì \n",
    "\n",
    "Indeed **if we can calculate $Eigen-vectors$ and $Eigen-values$, we can calculate the matrix decomposition that SVD gives us**\n",
    "\n",
    "The whole idea is rather simple: \n",
    "\n",
    "$$AX = \\lambda X$$\n",
    "\n",
    "Where: \n",
    "\n",
    "* $A$ is a matrix \n",
    "* $X$ is an $Eigen-vector$ \n",
    "* $\\lambda$ is an $Eigen-value$ \n",
    "\n",
    "With this, you can use simple linear algebra to determine each matrix $U, \\Sigma$ and $V^\\intercal$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!(https://vimeo.com/486594815)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Deduct PCA ü§†ü§†\n",
    "\n",
    "Now that we know how to find $U$, $\\Sigma$ and $V^\\intercal$ using $Eigen-vectors$ and $Eigen-values$, we can simple multiply $A$ (our initial matrix) by $U$ (our matrix of $Eigen-vectors$), to get our matrix of principal components! \n",
    "\n",
    "By the way, if you now get the covariance matrix of $AU$, this is what you get: \n",
    "\n",
    "<img src=\"https://full-stack-assets.s3.eu-west-3.amazonaws.com/images/New_covariance_matrix.png\" alt=\"new-covariance-matrix\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might be wondering how to reduce dimension? Well you simply need to check out the variance explained by each $PC$. \n",
    "\n",
    "Here $PC1$ explains $2.47$ of the variance which makes $\\frac{2.47}{2.47+0.03}= 99$% of the total variance. $PC1$ contains most of the information. Therefore, we can keep it! \n",
    "\n",
    "In practice, you can keep as many $PCs$ as you like depending on how much of the variance you want to keep. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!(https://vimeo.com/486599543)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Summary ‚úÖ\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <th>\n",
    "        Advantages\n",
    "    </th>\n",
    "    <th>\n",
    "        Flaws\n",
    "    </th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Reduce Redundancy\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "    Loss of information if you remove too many $PCs$ \n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Reduce overfitting\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "    $PCs$ are harder to interpret\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "<tr>\n",
    "<td>\n",
    "    Easier to visualize\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "    PCA works **only on quantitative data** \n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources üìöüìö\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=OELTJdaU8aA\" target=\"_blank\">Eigen values & Eigen Vectors</a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=a9jdQGybYmE\" target=\"_blank\">Lecture: Principal Component Analysis</a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=g-Hb26agBFg\" target=\"_blank\">Principal Component Analysis (PCA)</a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=PFDu9oVAE-g\" target=\"_blank\">Eigenvectors and eigenvalues | Essence of linear algebra, chapter 14</a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=FgakZw6K1QQ\" target=\"_blank\">StatQuest: Principal Component Analysis (PCA), Step-by-Step</a>\n",
    "\n",
    "*   <a href=\"https://medium.com/fintechexplained/what-are-eigenvalues-and-eigenvectors-a-must-know-concept-for-machine-learning-80d0fd330e47\" target=\"_blank\">What are Eigen Vectors and Eigen Values </a>\n",
    "\n",
    "* <a href=\"https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491\" target=\"_blank\">Machine Learning ‚Äî Singular Value Decomposition (SVD) & Principal Component Analysis (PCA)</a>\n",
    "\n",
    "* <a href=\"https://dzone.com/articles/understanding-what-is-principal-component-analysis\" target=\"_blank\">Understanding Principal Component Analysis (PCA)</a>\n",
    "\n",
    "* <a href=\"https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial#:~:text=PCA%20is%20predominantly%20used%20as,%2C%20bioinformatics%2C%20psychology%2C%20etc.\" target=\"_blank\">Principal Component Analysis Tutorial</a>\n",
    "\n",
    "* <a href=\"https://www.mygreatlearning.com/blog/covariance-vs-correlation/#:~:text=Covariance%20is%20when%20two%20variables,the%20change%20in%20another%20variable.\" target=\"_blank\">Covariance vs Correlation | Difference between correlation and covariance</a>\n",
    "\n",
    "* <a href=\"https://www.quora.com/When-and-where-do-we-use-PCA\" target=\"_blank\">When and Where do we use PCA</a>\n",
    "\n",
    "* <a href=\"https://blog.umetrics.com/what-is-principal-component-analysis-pca-and-how-it-is-used\" target=\"_blank\">What Is Principal Component Analysis (PCA) and How Is It Used?</a>\n",
    "\n",
    "* <a href=\"https://blog.exploratory.io/an-introduction-to-principal-component-analysis-pca-with-2018-world-soccer-players-data-810d84a14eab\" target=\"_blank\">An Introduction to Principal Component Analysis (PCA) with 2018 World Soccer Players Data</a>\n",
    "\n",
    "* <a href=\"https://towardsdatascience.com/linear-algebra-basics-dot-product-and-matrix-multiplication-2a7624942810#:~:text=Multiplication%20of%20two%20matrices%20involves%20dot%20products%20between%20rows%20of,first%20row%2C%20first%20column).\" target=\"_blank\">Linear Algebra Basics: Dot Product and Matrix Multiplication</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
